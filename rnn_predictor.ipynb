{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.concatenate([np.genfromtxt('../data/dis_forautoencoder_2012_dec_tokyo/day_{}.csv'.format(i), delimiter=',', dtype=np.int32)[:, 1:] for i in range(1, 32)], axis=0)\n",
    "np.random.shuffle(data)\n",
    "\n",
    "data_size = data.shape[0]\n",
    "train_data = data[:int(data_size * 0.8)]\n",
    "valid_data = data[int(data_size * 0.8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNPredictor(nn.Module):\n",
    "    def __init__(self, num_loc, embedding_dim, hidden_dim, n_layers):\n",
    "        \n",
    "        super(RNNPredictor, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_loc, embedding_dim)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, num_layers=n_layers, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_dim, num_loc - 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        output, _ = self.gru(embedded, None)\n",
    "        return self.out(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_LOC = 1443\n",
    "EMBEDDING_DIM = 64\n",
    "HIDDEN_DIM = 256\n",
    "N_LAYERS = 2\n",
    "\n",
    "rnn_predictor = RNNPredictor(NUM_LOC, EMBEDDING_DIM, HIDDEN_DIM, N_LAYERS).cuda()\n",
    "optimizer = torch.optim.SGD(rnn_predictor.parameters(), lr=1e-2, momentum=0.9)\n",
    "optim_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, [2, 4, 6, 8], gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria = nn.CrossEntropyLoss(ignore_index=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 001, 100.0%, avg_loss = 2.0857\n",
      "Iteration 001, avg_loss = 2.0276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fan/anaconda3/envs/py3.6/lib/python3.6/site-packages/torch/serialization.py:158: UserWarning: Couldn't retrieve source code for container of type RNNPredictor. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 002, 100.0%, avg_loss = 2.0161\n",
      "Iteration 002, avg_loss = 2.0141\n",
      "Iteration 003, 100.0%, avg_loss = 2.0078\n",
      "Iteration 003, avg_loss = 2.0065\n",
      "Iteration 004, 100.0%, avg_loss = 2.0028\n",
      "Iteration 004, avg_loss = 2.0053\n",
      "Iteration 005, 100.0%, avg_loss = 2.0020\n",
      "Iteration 005, avg_loss = 2.0045\n",
      "Iteration 006, 100.0%, avg_loss = 2.0016\n",
      "Iteration 006, avg_loss = 2.0044\n",
      "Iteration 007, 100.0%, avg_loss = 2.0014\n",
      "Iteration 007, avg_loss = 2.0043\n",
      "Iteration 008, 19.6%, avg_loss = 1.9997\r"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "for it in range(1, 11):\n",
    "    optim_scheduler.step()\n",
    "    np.random.shuffle(train_data)\n",
    "    avg_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    cnt = 0\n",
    "    for i in range(0, train_data.shape[0], BATCH_SIZE):\n",
    "        cnt += 1\n",
    "        perc = i / train_data.shape[0] * 100.0\n",
    "        x_batch = train_data[i: min(train_data.shape[0], i + BATCH_SIZE)]\n",
    "        y_batch = Variable(torch.LongTensor(x_batch[:, 1:])).cuda()\n",
    "        x_batch = Variable(torch.LongTensor(x_batch[:, :-1])).cuda()\n",
    "        y_pred = rnn_predictor(x_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criteria(y_pred.view(-1, NUM_LOC - 2), torch.clamp(y_batch - 2, min=-1).view(-1))\n",
    "        loss.backward()\n",
    "        avg_loss += loss.data[0]\n",
    "        optimizer.step()\n",
    "        print('Iteration {:03d}, {:3.1f}%, avg_loss = {:.4f}'.format(it, perc, avg_loss / cnt), end='\\r')\n",
    "    print('')\n",
    "    \n",
    "    cnt = 0\n",
    "    for i in range(0, valid_data.shape[0], BATCH_SIZE):\n",
    "        cnt += 1\n",
    "        x_batch = valid_data[i: min(valid_data.shape[0], i + BATCH_SIZE)]\n",
    "        y_batch = Variable(torch.LongTensor(x_batch[:, 1:])).cuda()\n",
    "        x_batch = Variable(torch.LongTensor(x_batch[:, :-1])).cuda()\n",
    "        y_pred = rnn_predictor(x_batch)\n",
    "        loss = criteria(y_pred.view(-1, NUM_LOC - 2), torch.clamp(y_batch - 2, min=-1).view(-1))\n",
    "        valid_loss += loss.data[0]\n",
    "    print('Iteration {:03d}, avg_loss = {:.4f}'.format(it, valid_loss / cnt))\n",
    "    torch.save(rnn_predictor, '../results/sadRNNPredictor/rnn_predictor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Conda PyTorch",
   "language": "python",
   "name": "py3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
